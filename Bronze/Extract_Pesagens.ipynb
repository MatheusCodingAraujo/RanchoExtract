{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab165ae-e448-4a5e-ba0b-3c41951c7c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'x-access-token-ws': 'd1bf4238656c2167c5997e4081b549eedd0a25622a18740d8ad1a8a31ee22b15'\n",
    "}\n",
    "\n",
    "# -------------------- EXTRAÇÃO ---------------------------------------\n",
    "url = 'https://api.irancho.com.br/api/animal/pesagens'\n",
    "response = requests.get(url, headers=headers)\n",
    "pesagens_list = response.json()\n",
    "df_pesagens_pandas = pd.json_normalize(pesagens_list, sep='.')\n",
    "print(f'pesagens {response.status_code}')\n",
    "\n",
    "# ✅ CONVERTER TUDO PARA STRING NO PANDAS\n",
    "df_pesagens_pandas = df_pesagens_pandas.astype(str)\n",
    "\n",
    "# Criar schema com tudo como String\n",
    "schema = StructType([\n",
    "    StructField(col, StringType(), True) \n",
    "    for col in df_pesagens_pandas.columns\n",
    "])\n",
    "\n",
    "df_pesagens = spark.createDataFrame(df_pesagens_pandas, schema=schema)\n",
    "df_pesagens.write.mode(\"overwrite\").saveAsTable(\"pesagens_raw\")\n",
    "\n",
    "print(\"✅ Tabela criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a08f1c8-4daf-4a65-a35d-a1d77a9f6770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    regexp_replace,\n",
    "    schema_of_json,\n",
    "    from_json,\n",
    "    explode\n",
    ")\n",
    "\n",
    "# 1) --- Troca aspas simples por aspas duplas\n",
    "df_clean = df_pesagens.withColumn(\n",
    "    \"rows_clean\",\n",
    "    regexp_replace(col(\"rows\"), \"'\", '\"')\n",
    ")\n",
    "\n",
    "# 2) --- Troca valores Python pelos correspondentes JSON\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    .withColumn(\"rows_clean\", regexp_replace(\"rows_clean\", \"None\", \"null\"))\n",
    "    .withColumn(\"rows_clean\", regexp_replace(\"rows_clean\", \"True\", \"true\"))\n",
    "    .withColumn(\"rows_clean\", regexp_replace(\"rows_clean\", \"False\", \"false\"))\n",
    ")\n",
    "\n",
    "# 3) --- Pegar um exemplo para inferir o schema JSON\n",
    "sample_json = df_clean.select(\"rows_clean\").first()[0]\n",
    "\n",
    "# 4) --- Inferir automaticamente o schema JSON\n",
    "json_schema = (\n",
    "    spark.range(1)\n",
    "         .select(schema_of_json(sample_json).alias(\"schema\"))\n",
    "         .first()[\"schema\"]\n",
    ")\n",
    "\n",
    "# 5) --- Converter STRING → ARRAY<STRUCT>\n",
    "df_array = df_clean.withColumn(\n",
    "    \"rows_array\",\n",
    "    from_json(col(\"rows_clean\"), json_schema)\n",
    ")\n",
    "\n",
    "# 6) --- Explodir a lista\n",
    "df_exploded = df_array.select(explode(\"rows_array\").alias(\"row\"))\n",
    "\n",
    "# 7) --- Expandir os campos internos\n",
    "df_final = df_exploded.select(\"row.*\")\n",
    "\n",
    "df_exploded = df_final.select(\n",
    "    explode(\"pesagens\").alias(\"pesagem\")\n",
    ")\n",
    "\n",
    "df_pesagens_final=df_exploded.select(\"pesagem.*\")\n",
    "\n",
    "df_pesagens_final.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"pesagens_raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90f7c5f-2b95-4e10-a715-cd4524a63dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table workspace.irancho.pesagens as \n",
    "  select id_animal, dt_pesagem, id_assinatura, id_pesagem, id_responsavel, ps_animal, nu_gmds, nu_gmdup, dt_alteracao, dt_criacao, id_tipo_pesagem\n",
    "from pesagens_raw;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5382802099605959,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Extract_Pesagens",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
